
# Name your project! Project names should contain only lowercase characters
# and underscores. A good package name should reflect your organization's
# name or the intended use of these models

name: 'smauto'
version: '1.0.0'
config-version: 2




#define project variable here 
#access variable>>dbt.config.get("name") 

vars:
 
 ##call macrose here , it work for sql model.
  as_of_date_s: " {{ get_current_date() }}"      #{{ var("as_of_date_s") }} (in sql model)
  statuss: "OFF"                    #ues in call_variable_in model.py

  dd: "sunil"                          #"{{get_current()}}"
  name: "sunil rathod"                 #  '{{ var("name") }}'
  todayss: "2024-05-15"                #  '{{ var("todayss") }}'
  process_date: "2024-05-16"           #  '{{ var("process_date") }}'         #for model refine_sale_data.sql

  


  target_schema: "my_python_schema"
  #sql_date: '''{{var("as_of_Date")}}'''     # we are getting variable from airflow
  
# dbt run --vars 'key:value'


# This setting configures which "profile" dbt uses for this project.
profile: 'smauto'

# These configurations specify where dbt should look for different types of files.
# The `model-paths` config, for example, states that models in this project can be
# found in the "models/" directory. You probably won't need to change these!
model-paths: ["models"]
analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

clean-targets:         # directories to be removed by `dbt clean`
  - "target"
  - "dbt_packages"


#>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> hooks imp BEFORE START AND END RUN DBT MODULE 
#Logging and Auditing, Environment Setup and DROP TEMP TABLE, Notification and Alerting
#


on-run-start: |
  CREATE TABLE IF NOT EXISTS demo.public.start_tmp_staging (id INT, name STRING);

on-run-end: |
  CREATE TABLE IF NOT EXISTS demo.public.end_tmp_staging (id INT, name STRING);
  DROP TABLE if EXISTS  demo.public.start_tmp_staging; 


# DROP view if EXISTS  demo.OUTPUT_DATA_SCHEMA.DROP_THIS_TABLE_USING_HOOKS;  

# real use case drop table which created temparary, it will not work, we can achibe above using 



# on-run-start: "insert into DEMO.PUBLIC.RAW_EMP (EMPLOYEE_ID,FIRST_NAME,LAST_NAME) values (112,'sunil','rr')"
# on-run-end: "insert into DEMO.PUBLIC.RAW_EMP (EMPLOYEE_ID,FIRST_NAME,LAST_NAME) values (112,'sunil','ee')"


# Configuring models
# Full documentation: https://docs.getdbt.com/docs/configuring-models

# In this example config, we tell dbt to build all models in the example/
# directory as views. These settings can be overridden in the individual model
# files using the `{{ config(...) }}` macro.

models:

# # BEFORE START AND END OF each MODULE it run
# >>>>>>>>>>>>>>>>Pre-hooks:
# Log the start of the model run.
# Clean up a temporary table.

# >>>>>>>>> Post-hooks:
# Log the end of the model run.
# Update a metadata table with the last run time.
# data validation

  # pre-hook: "insert into DEMO.PUBLIC.RAW_EMP (EMPLOYEE_ID,SALARY,DEPARTMENT) values (112,2222,'IT')"
  # POST-HOOK: "insert into DEMO.PUBLIC.RAW_EMP (EMPLOYEE_ID,SALARY,DEPARTMENT) values (112,2222,'IT')"

  # pre-hook: "INSERT INTO audit_log (model_name, action, timestamp) VALUES ('raw_customers_join1', 'start', current_timestamp)"
  # post-hook: "INSERT INTO audit_log (model_name, action, timestamp) VALUES ('raw_customers_join1', 'end', current_timestamp)"
 
  
  # pre-hook: 
  #     - "{{ 'INSERT INTO audit_log (model_name, action, timestamp) VALUES (\"{}\", \"start\", current_timestamp)'.format(this.name) }}"
  # # post-hook:
  #     - "{{ 'INSERT INTO audit_log (model_name, action, timestamp) VALUES (\"{}\", \"end\", current_timestamp)'.format(this.name) }}"

 
    
    
# +transient: false          # :: it create permanent table in snowflake.        
  smauto:
  

 
    # Config indicated by + and applies to all files under models/example/
    raw_data:
      +materialized: view
      +schema: raw_data_SCHEMA  # by defining schema here,we can override the custome schema inside profile.yml file

    conformed_data:  
      +materialized: view
      +schema: conformed_data_SCHEMA

    publish_data:
      +materialized: table 
      +transient: false  


    use_macros:
      +materialized: view   

    new_model:  
      +materialized: view   

#below model override the target schema and create the permanent table
    override_target_schema: 
      
      +schema: override_schema_practice 
      +materialized: table  
      +transient: false                   

    raw_analysis: 
      schema: raw_analysis_schema
      +materialized: table  
      +transient: false               #+transient: false    :: it create permanent table in snowflake.



    1_secadv_1_conformed:
      +schema: secadv_1_conformed
      +materialized: view  

#changing the output schema for python dbt file:::
    python_dbt_models:
      +schema: python_dbt_models_schema
      +materialized: table 
     

    


# {{ config(materialized="incremental") }}   -- by using this old and new upded record keep 


#four type of materialized table , view , ephemeral, incremental 
#python support table and view     
#views are automatically updated and table not automatically update need to run again
#table is fast for query
#ephemeral not take space in warehouse, we can use this in other model, debug difficult
#incremental : insert and update record (table)
#delete+insert >>>this is usefull to get all upded record. hen we are not deleting it wil keep all old record and new also
#dbt 
##by defalut table is transient, by making false it create parmanent table 
  

###############################Hooks

# pre-hook:  executed before model,seed,snapshot
# post-hook: executed after model,seed,snapshot  

#on-run-start: executed at the start of dbt run, dbt test, dbt seed , dbt snapshot

#on-run-end: executed at the end of dbt run, dbt test, dbt seed , dbt snapshot
      



